{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bestlabmct/miniconda3/envs/py38belajarpt/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATHROOT=\"dataset/Samples\"\n",
    "FPS=125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tersedia GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Tersedia GPU\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Tersedia MPS Apple Silicon\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Tersedia CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate all files using glob\n",
    "files = glob.glob(PATHROOT + \"/*.csv\")\n",
    "files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check len of every file\n",
    "# for file in files:\n",
    "#     # load csv into numpy\n",
    "#     data = np.genfromtxt(file, delimiter=',')\n",
    "#     data = data.T    \n",
    "#     # print(file, len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create collate_fn using RNN pad sequence\n",
    "def collate_fn(data):\n",
    "    # data is a list of tuples\n",
    "    # each tuple is (data, label)\n",
    "    # sort the data list by label\n",
    "    data.sort(key=lambda x: len(x[0]), reverse=True)\n",
    "    # seperate data and label\n",
    "    # data, = zip(*data)\n",
    "    # merge data (from tuple of 1D tensor to 2D tensor)\n",
    "    data = torch.nn.utils.rnn.pad_sequence(data, batch_first=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We have 500 subjects, each subject has 3 column of signal.\n",
    "Each subjects has different number of samples.\n",
    "\n",
    "\n",
    "We want to create a dataset class that can be used by PyTorch Dataloader.\n",
    "\"\"\"\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.files = glob.glob(root_dir + \"/*.csv\")\n",
    "        self.files.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # load csv into numpy\n",
    "        data = np.genfromtxt(self.files[idx], delimiter=',')\n",
    "        data = data.T\n",
    "        # get label from filename\n",
    "        # label = int(self.files[idx].split('/')[-1].split('_')[0])\n",
    "        # convert to tensor\n",
    "        data = torch.from_numpy(data).float()\n",
    "        # label = torch.tensor(label)\n",
    "        # apply transform\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "physio_dataset = Dataset(PATHROOT)\n",
    "print(len(physio_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch Dataloader using Dataset\n",
    "traindl = torch.utils.data.DataLoader(physio_dataset, batch_size=2, shuffle=True, collate_fn=collate_fn, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try to iterate through dataloader, only 1 batch\n",
    "# for i_batch, sample_batched in enumerate(traindl):\n",
    "#     print(i_batch, sample_batched.size())\n",
    "#     if i_batch == 0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample = next(iter(traindl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(sample.shape)\n",
    "# print(sample[1][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Remember, for each subject, we have 3 column signal data.\n",
    "First is PPG, Second is ABP, Third is ECG.\n",
    "We want to make a seq2seq model that can predict ABP from PPG and ECG.\n",
    "\"\"\"\n",
    "\n",
    "# create ABP sequence encoder\n",
    "class ABPEncoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout):\n",
    "        super(ABPEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # set initial hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        # forward propagate LSTM\n",
    "        out, (h_n, c_n) = self.lstm(x, (h0, c0))\n",
    "        # return the final hidden state and cell state\n",
    "        return h_n, c_n\n",
    "\n",
    "# create ABP sequence decoder\n",
    "class ABPDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(ABPDecoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x, h_n, c_n):\n",
    "        # forward propagate LSTM\n",
    "        out, (h_n, c_n) = self.lstm(x, (h_n, c_n))\n",
    "        # decode the hidden state of the last time step\n",
    "        out = self.fc(out)\n",
    "        return out, (h_n, c_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now, combine Encoder and Decoder into Seq2Seq model.\n",
    "\"\"\"\n",
    "\n",
    "# create seq2seq model\n",
    "# Remember, we use batch_first=True\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # set initial states\n",
    "        h_n, c_n = self.encoder(x)\n",
    "        # print(f\"Encoder output: {h_n.shape}, {c_n.shape}\")\n",
    "        # create initial input (start with zeros)\n",
    "        decoder_input = torch.zeros(x.size(0), 1, 1).to(device)\n",
    "        # init output tensor\n",
    "        # print(x.size(1), x.size(0))\n",
    "        outputs = torch.zeros(x.size(1), x.size(0),  1).to(device)\n",
    "        # decode hidden state of last time step\n",
    "        for t in range(x.size(1)):\n",
    "            decoder_output, (h_n, c_n) = self.decoder(decoder_input, h_n, c_n)\n",
    "            # print(f\"Decoder output: {decoder_output.shape}, {h_n.shape}, {c_n.shape}\")\n",
    "            # decoder_output = decoder_output.squeeze(1)\n",
    "            outputs[t] = decoder_output.squeeze(1)\n",
    "            decoder_input = decoder_output\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "# class Seq2Seq(nn.Module):\n",
    "#     def __init__(self, encoder, decoder):\n",
    "#         super(Seq2Seq, self).__init__()\n",
    "#         self.encoder = encoder\n",
    "#         self.decoder = decoder\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         # set initial states\n",
    "#         h_n, c_n = self.encoder(x)\n",
    "#         print(f\"Encoder output: {h_n.shape}, {c_n.shape}\")\n",
    "#         # create initial input (start with zeros)\n",
    "#         decoder_input = torch.zeros(1, x.size(0), 1).to(device)\n",
    "#         # init output tensor\n",
    "#         outputs = torch.zeros(x.size(1), x.size(0), 1).to(device)\n",
    "#         # decode hidden state of last time step\n",
    "#         for t in range(x.size(1)):\n",
    "#             decoder_output, (h_n, c_n) = self.decoder(decoder_input, h_n, c_n)\n",
    "#             # store output\n",
    "#             outputs[t] = decoder_output\n",
    "#             # make current output next decoder input\n",
    "#             decoder_input = decoder_output\n",
    "        \n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "INPUT_DIM = 2\n",
    "OUTPUT_DIM = 1\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.5\n",
    "DEC_DROPPOUT = 0.5\n",
    "\n",
    "# create encoder and decoder\n",
    "encoder = ABPEncoder(INPUT_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "decoder = ABPDecoder(1, HID_DIM, OUTPUT_DIM, N_LAYERS, DEC_DROPPOUT)\n",
    "\n",
    "# create seq2seq model\n",
    "model = Seq2Seq(encoder, decoder).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): ABPEncoder(\n",
       "    (lstm): LSTM(2, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  )\n",
       "  (decoder): ABPDecoder(\n",
       "    (lstm): LSTM(1, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
       "    (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize weights\n",
    "\n",
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Optimizer, Loss Function, and Evaluation Metric\n",
    "\n",
    "opt = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "import torchmetrics\n",
    "\n",
    "# use RMSE as evaluation metric. USe MSE then pass argument False\n",
    "metric  = torchmetrics.MeanSquaredError(squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we want to train the model. Before that, we create the seq2seq training loop.\n",
    "\"\"\"\n",
    "\n",
    "# seq2seq training loop for the encoder-decoder model\n",
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(iterator):\n",
    "        # get input and targets and get to cuda. Remember, input is on [:,:,0] and [:,:,2], while target is on [:,:,1]\n",
    "        input = batch[:, :, [0,2]].to(device)\n",
    "        target = batch[:, :, 1].to(device)\n",
    "        # forward prop\n",
    "        output = model(input)\n",
    "        # output = [batch size, seq len, output dim]\n",
    "        # target = [batch size, seq len, output dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output.contiguous().view(-1, output_dim)\n",
    "        target = target.contiguous().view(-1, output_dim)\n",
    "        # loss and backprop\n",
    "        loss = criterion(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # clip gradient norms\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        # record loss\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Batch {i+1} loss: {loss.item()}\")\n",
    "\n",
    "    return epoch_loss / len(iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 loss: 4976.427734375\n",
      "Batch 2 loss: 2585.648681640625\n",
      "Batch 3 loss: 4282.90771484375\n",
      "Batch 4 loss: 8729.7021484375\n",
      "Batch 5 loss: 2841.92919921875\n",
      "Batch 6 loss: 6836.955078125\n",
      "Batch 7 loss: 6748.6259765625\n",
      "Batch 8 loss: 4020.52490234375\n",
      "Batch 9 loss: 1933.9794921875\n",
      "Batch 10 loss: 3138.328857421875\n",
      "Batch 11 loss: 7673.94775390625\n",
      "Batch 12 loss: 6654.3193359375\n",
      "Batch 13 loss: 5221.34130859375\n",
      "Batch 14 loss: 5035.107421875\n",
      "Batch 15 loss: 4035.20458984375\n",
      "Batch 16 loss: 3272.1083984375\n",
      "Batch 17 loss: 3030.54638671875\n",
      "Batch 18 loss: 3835.49365234375\n",
      "Batch 19 loss: 4351.892578125\n",
      "Batch 20 loss: 5391.20654296875\n",
      "Batch 21 loss: 5505.16015625\n",
      "Batch 22 loss: 2305.82373046875\n",
      "Batch 23 loss: 2081.389892578125\n",
      "Batch 24 loss: 4715.22265625\n",
      "Batch 25 loss: 2035.69873046875\n",
      "Batch 26 loss: 6326.08984375\n",
      "Batch 27 loss: 3933.836181640625\n",
      "Batch 28 loss: 3327.7783203125\n",
      "Batch 29 loss: 2696.89892578125\n",
      "Batch 30 loss: 2808.564453125\n",
      "Batch 31 loss: 3766.423583984375\n",
      "Batch 32 loss: 2209.57568359375\n",
      "Batch 33 loss: 4928.603515625\n",
      "Batch 34 loss: 2685.111083984375\n",
      "Batch 35 loss: 3798.611083984375\n",
      "Batch 36 loss: 4238.146484375\n",
      "Batch 37 loss: 3776.822265625\n",
      "Batch 38 loss: 3486.74462890625\n",
      "Batch 39 loss: 2513.3037109375\n",
      "Batch 40 loss: 4567.26611328125\n",
      "Batch 41 loss: 3961.677001953125\n",
      "Batch 42 loss: 1436.5885009765625\n",
      "Batch 43 loss: 2890.95751953125\n",
      "Batch 44 loss: 3767.80029296875\n",
      "Batch 45 loss: 2959.591064453125\n",
      "Batch 46 loss: 3096.42041015625\n",
      "Batch 47 loss: 2750.224365234375\n",
      "Batch 48 loss: 2441.2861328125\n",
      "Batch 49 loss: 2847.8330078125\n",
      "Batch 50 loss: 2295.889892578125\n",
      "Batch 51 loss: 4643.39599609375\n",
      "Batch 52 loss: 2120.84619140625\n",
      "Batch 53 loss: 3986.652587890625\n",
      "Batch 54 loss: 4253.72607421875\n",
      "Batch 55 loss: 2337.06591796875\n",
      "Batch 56 loss: 3106.90966796875\n",
      "Batch 57 loss: 3867.67626953125\n",
      "Batch 58 loss: 2800.817626953125\n",
      "Batch 59 loss: 2612.281982421875\n",
      "Batch 60 loss: 2692.411376953125\n",
      "Batch 61 loss: 2337.744873046875\n",
      "Batch 62 loss: 1960.7305908203125\n",
      "Batch 63 loss: 2391.051025390625\n",
      "Batch 64 loss: 3466.771240234375\n",
      "Batch 65 loss: 3171.358154296875\n",
      "Batch 66 loss: 2989.896728515625\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train Process\n",
    "\"\"\"\n",
    "\n",
    "# Train The Model\n",
    "\n",
    "epochs = 10\n",
    "clip = 1\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(model, traindl, opt, criterion, clip)\n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 10, 2)\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38belajarpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "70e73a41c653c3ff2c4eb4196dfcfae1e64ffcb4fa97f2ff95a868530ad91676"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
